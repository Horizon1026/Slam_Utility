#ifndef _GAUSSIAN_MIXTURE_MODEL_H_
#define _GAUSSIAN_MIXTURE_MODEL_H_

#include <algorithm>
#include <cmath>
#include <numeric>
#include <random>
#include <vector>

#include "basic_type.h"
#include "normal_distribution.h"

namespace slam_utility {

/* Class GaussianMixtureModel Declaration. */
template <int32_t kDimension>
class GaussianMixtureModel {

public:
    struct Options {
        uint32_t kNumberOfNormalDistributions = 10;
        uint32_t kMaxIterations = 100;
        float kMaxLogLikelihoodChangeTolerance = 1e-4f;
    };

public:
    GaussianMixtureModel() = default;
    virtual ~GaussianMixtureModel() = default;

    // Operations.
    void Reset();
    bool DirectlyFitDistribution(const std::vector<TVec<float, kDimension>> &points);

    // Reference for member variables.
    Options &options() { return options_; }
    std::vector<float> &weights() { return weights_; }
    std::vector<NormalDistribution<kDimension>> &distributions() { return normal_distributions_; }
    // Const reference for member variables.
    const Options &options() const { return options_; }
    const std::vector<float> &weights() const { return weights_; }
    const std::vector<NormalDistribution<kDimension>> &distributions() const { return normal_distributions_; }

private:
    Options options_;
    std::vector<NormalDistribution<kDimension>> normal_distributions_;
    std::vector<float> weights_;

};

/* Class GaussianMixtureModel Implementation. */
template <int32_t kDimension>
void GaussianMixtureModel<kDimension>::Reset() {
    normal_distributions_.clear();
    normal_distributions_.resize(options_.kNumberOfNormalDistributions);
    weights_.assign(options_.kNumberOfNormalDistributions, 1.0f / options_.kNumberOfNormalDistributions);
}

template <int32_t kDimension>
bool GaussianMixtureModel<kDimension>::DirectlyFitDistribution(const std::vector<TVec<float, kDimension>> &points) {
    RETURN_FALSE_IF(points.empty());
    const uint32_t num_of_points = points.size();
    const uint32_t num_of_kernels = options_.kNumberOfNormalDistributions;
    Reset();

    // 1. K-means Initialization (Hard assignment)
    // Randomly pick initial centroids from the dataset.
    std::vector<uint32_t> indices(num_of_points);
    std::iota(indices.begin(), indices.end(), 0);
    std::random_device rd;
    std::mt19937 g(rd());
    std::shuffle(indices.begin(), indices.end(), g);

    for (uint32_t k = 0; k < num_of_kernels; ++k) {
        normal_distributions_[k].mid_point() = points[indices[k % num_of_points]];
        // Initialize covariance with a small identity matrix
        normal_distributions_[k].covariance() = TMat<float, kDimension, kDimension>::Identity() * 0.1f;
        normal_distributions_[k].SynchronizeToAdvancedParameters();
    }

    // A few K-means iterations to get a good starting point for EM.
    for (uint32_t iter = 0; iter < 10; ++iter) {
        std::vector<TVec<float, kDimension>> new_means(num_of_kernels, TVec<float, kDimension>::Zero());
        std::vector<uint32_t> counts(num_of_kernels, 0);
        // Assign each point to the nearest kernel.
        for (const auto &p : points) {
            uint32_t best_k = 0;
            float min_dist = std::numeric_limits<float>::max();
            // Find the nearest kernel for the current point.
            for (uint32_t k = 0; k < num_of_kernels; ++k) {
                const float dist = (p - normal_distributions_[k].mid_point()).squaredNorm();
                if (dist < min_dist) {
                    min_dist = dist;
                    best_k = k;
                }
            }
            new_means[best_k] += p;
            counts[best_k]++;
        }

        // Update the mean of each kernel.
        for (uint32_t k = 0; k < num_of_kernels; ++k) {
            CONTINUE_IF(counts[k] == 0);
            normal_distributions_[k].mid_point() = new_means[k] / static_cast<float>(counts[k]);
        }
    }

    // 2. EM Algorithm (Soft assignment)
    // Responsibility matrix: gamma(i, k) is the probability that point i was generated by kernel k.
    std::vector<std::vector<float>> responsibilities(num_of_points, std::vector<float>(num_of_kernels));
    float last_log_likelihood = -std::numeric_limits<float>::max();
    for (uint32_t iter = 0; iter < options_.kMaxIterations; ++iter) {
        // --- E-step: Compute responsibilities ---
        float current_log_likelihood = 0.0f;
        for (uint32_t i = 0; i < num_of_points; ++i) {
            float sum_prob = 0.0f;
            for (uint32_t k = 0; k < num_of_kernels; ++k) {
                const float prob = weights_[k] * normal_distributions_[k].ProbabilityDensityFunction(points[i]);
                responsibilities[i][k] = prob;
                sum_prob += prob;
            }

            if (sum_prob > kZeroFloat) {
                for (uint32_t k = 0; k < num_of_kernels; ++k) {
                    responsibilities[i][k] /= sum_prob;
                }
                current_log_likelihood += std::log(sum_prob);
            } else {
                // Handle cases where a point has zero probability for all kernels (outliers).
                for (uint32_t k = 0; k < num_of_kernels; ++k) {
                    responsibilities[i][k] = 1.0f / static_cast<float>(num_of_kernels);
                }
            }
        }

        // Convergence check based on log-likelihood change.
        BREAK_IF(std::fabs(current_log_likelihood - last_log_likelihood) < options_.kMaxLogLikelihoodChangeTolerance);
        last_log_likelihood = current_log_likelihood;

        // --- M-step: Update distribution parameters ---
        for (uint32_t k = 0; k < num_of_kernels; ++k) {
            // Effective number of points assigned to kernel k.
            float nk = 0.0f;
            TVec<float, kDimension> new_mu = TVec<float, kDimension>::Zero();
            for (uint32_t i = 0; i < num_of_points; ++i) {
                nk += responsibilities[i][k];
                new_mu += responsibilities[i][k] * points[i];
            }

            const float regularization = 1e-4f;
            if (nk > regularization) {
                // Update mixing weight.
                weights_[k] = nk / static_cast<float>(num_of_points);
                // Update mean.
                normal_distributions_[k].mid_point() = new_mu / nk;
                // Update covariance
                TMat<float, kDimension, kDimension> new_cov = TMat<float, kDimension, kDimension>::Zero();
                for (uint32_t i = 0; i < num_of_points; ++i) {
                    const TVec<float, kDimension> diff = points[i] - normal_distributions_[k].mid_point();
                    new_cov += responsibilities[i][k] * diff * diff.transpose();
                }
                // Add regularization to prevent the covariance matrix from becoming singular.
                normal_distributions_[k].covariance() = new_cov / nk + TMat<float, kDimension, kDimension>::Identity() * regularization;
                normal_distributions_[k].num_of_points() = static_cast<uint32_t>(nk);
                // Pre-compute inverse covariance and other parameters for PDF calculation
                normal_distributions_[k].SynchronizeToAdvancedParameters();
            } else {
                // If a kernel has no points, re-initialize it randomly.
                normal_distributions_[k].mid_point() = points[g() % num_of_points];
                normal_distributions_[k].covariance() = TMat<float, kDimension, kDimension>::Identity() * 0.1f;
                normal_distributions_[k].SynchronizeToAdvancedParameters();
                weights_[k] = 1.0f / static_cast<float>(num_of_points);
            }
        }
    }

    return true;
}

}  // namespace slam_utility

#endif  // end of _GAUSSIAN_MIXTURE_MODEL_H_
